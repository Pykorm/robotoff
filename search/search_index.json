{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Robotoff Robotoff is a service managing potential Open Food Facts updates (also known as insights ) and predictions (which can then be combined to generate an insight). These insights include a growing set of facts, including: the product category, weight, brand, packager codes and expiration date some of its labels abusive pictures (selfies) rotated pictures ingredient spellchecking Robotoff provides an API to: Fetch insights Annotate an insight (accept or reject) Once generated, the insights can be applied automatically, or after a manual validation if necessary. A scheduler regularly marks insights for automatic annotation and sends the update to Open Food Facts. Robotoff works together with Product Opener , the Core server of Open Food Facts (in Perl, which can also be installed locally using Docker) and the Open Food Facts apps (which can work with your local instance after enabling dev mode) Documentation: https://openfoodfacts.github.io/robotoff Source code: https://github.com/openfoodfacts/robotoff Open Food Facts: https://world.openfoodfacts.org Overview To get a better understanding on how Robotoff works, go to Architecture . If you want to help, go to Contributing . Robotoff can be used as... an online API a CLI tool a Python package If you need to deploy or maintain Robotoff, Maintenance is the way to go. NOTE: This documentation tries to follow as much as possible the documentation system from Di\u00e1taxis . Licence Robotoff is licensed under the AGPLv3.","title":"Robotoff"},{"location":"#robotoff","text":"Robotoff is a service managing potential Open Food Facts updates (also known as insights ) and predictions (which can then be combined to generate an insight). These insights include a growing set of facts, including: the product category, weight, brand, packager codes and expiration date some of its labels abusive pictures (selfies) rotated pictures ingredient spellchecking Robotoff provides an API to: Fetch insights Annotate an insight (accept or reject) Once generated, the insights can be applied automatically, or after a manual validation if necessary. A scheduler regularly marks insights for automatic annotation and sends the update to Open Food Facts. Robotoff works together with Product Opener , the Core server of Open Food Facts (in Perl, which can also be installed locally using Docker) and the Open Food Facts apps (which can work with your local instance after enabling dev mode) Documentation: https://openfoodfacts.github.io/robotoff Source code: https://github.com/openfoodfacts/robotoff Open Food Facts: https://world.openfoodfacts.org","title":"Robotoff"},{"location":"#overview","text":"To get a better understanding on how Robotoff works, go to Architecture . If you want to help, go to Contributing . Robotoff can be used as... an online API a CLI tool a Python package If you need to deploy or maintain Robotoff, Maintenance is the way to go. NOTE: This documentation tries to follow as much as possible the documentation system from Di\u00e1taxis .","title":"Overview"},{"location":"#licence","text":"Robotoff is licensed under the AGPLv3.","title":"Licence"},{"location":"explanations/questions/","text":"Question format proposal After loading a product, the client (web, iOS, Android) will request the Robotoff server a question (on /api/v1/questions/{barcode} ). 1 A question includes an insight and metadata such that the client knows how to display the question and what kind of input is expected from the user. All data returned by Robotoff is localized with respect to the lang parameter provided by the client. Question formats For all formats, a I don't know button will offer the user the possibility to leave without answering the question. Addition: Binary choice ( add-binary ) Add a fact about a product, by accepting (1) or rejecting (0) the insight. For instance: add a new label ( en:organic ) add a new packager code ( EMB 52052B ) Add a new category ( en:pastas ) Format type (str, required) - The question type ( add-binary ) question (str, required) - The question, in the user locale value (str, optional) - The suggested value for the field image_url (str, optional) - An image to display insight_id (str, required) - ID of the insight value or image_url cannot be both missing. Examples { \"type\" : \"add-binary\" , \"question\" : \"Does the product belong to this category ?\" , \"value\" : \"Pastas\" , \"insight_id\" : \"{INSIGHT_ID}\" , \"insight_type\" : \"category\" , \"barcode\" : \"{BARCODE}\" } { \"type\" : \"add-binary\" , \"question\" : \"Does the product have this label?\" , \"value\" : \"EU Organic\" , \"image_url\" : \"https://static.openfoodfacts.org/images/lang/fr/labels/bio-europeen.135x90.png\" , \"source_image_url\" : \"https://static.openfoodfacts.org/images/products/542/503/557/7122/1.jpg\" , \"insight_id\" : \"{INSIGHT_ID}\" , \"insight_type\" : \"label\" , \"barcode\" : \"{BARCODE}\" } The image_url can be used to display an image in the question interface, such as labels (IGP, organic,...). The source_image_url is the URL of the image from which the insight was extracted, if any. The client returns the insight ID and the annotation (0, -1 or 1). 2 see robotoff.app.api.ProductQuestionsResource \u21a9 see robotoff.app.api.AnnotateInsightResource \u21a9","title":"Question format proposal"},{"location":"explanations/questions/#question-format-proposal","text":"After loading a product, the client (web, iOS, Android) will request the Robotoff server a question (on /api/v1/questions/{barcode} ). 1 A question includes an insight and metadata such that the client knows how to display the question and what kind of input is expected from the user. All data returned by Robotoff is localized with respect to the lang parameter provided by the client.","title":"Question format proposal"},{"location":"explanations/questions/#question-formats","text":"For all formats, a I don't know button will offer the user the possibility to leave without answering the question.","title":"Question formats"},{"location":"explanations/questions/#addition-binary-choice-add-binary","text":"Add a fact about a product, by accepting (1) or rejecting (0) the insight. For instance: add a new label ( en:organic ) add a new packager code ( EMB 52052B ) Add a new category ( en:pastas )","title":"Addition: Binary choice (add-binary)"},{"location":"explanations/questions/#format","text":"type (str, required) - The question type ( add-binary ) question (str, required) - The question, in the user locale value (str, optional) - The suggested value for the field image_url (str, optional) - An image to display insight_id (str, required) - ID of the insight value or image_url cannot be both missing.","title":"Format"},{"location":"explanations/questions/#examples","text":"{ \"type\" : \"add-binary\" , \"question\" : \"Does the product belong to this category ?\" , \"value\" : \"Pastas\" , \"insight_id\" : \"{INSIGHT_ID}\" , \"insight_type\" : \"category\" , \"barcode\" : \"{BARCODE}\" } { \"type\" : \"add-binary\" , \"question\" : \"Does the product have this label?\" , \"value\" : \"EU Organic\" , \"image_url\" : \"https://static.openfoodfacts.org/images/lang/fr/labels/bio-europeen.135x90.png\" , \"source_image_url\" : \"https://static.openfoodfacts.org/images/products/542/503/557/7122/1.jpg\" , \"insight_id\" : \"{INSIGHT_ID}\" , \"insight_type\" : \"label\" , \"barcode\" : \"{BARCODE}\" } The image_url can be used to display an image in the question interface, such as labels (IGP, organic,...). The source_image_url is the URL of the image from which the insight was extracted, if any. The client returns the insight ID and the annotation (0, -1 or 1). 2 see robotoff.app.api.ProductQuestionsResource \u21a9 see robotoff.app.api.AnnotateInsightResource \u21a9","title":"Examples"},{"location":"how-to-guides/test-and-debug/","text":"Test and debug This documentation intends to: 1) Familiarize you on running our test cases. 2) Give you some tips from getting stuck if you are working only on Robotoff and don't need complete installation of Open Food Facts server. How to generate data? Robotoff is an API that pulls prediction data, annotation data, product data, nutrition data from MongoDB database in Open Food Facts server. If your development instance is not connected to a product-opener instance (which happens automatically if you have a running product-opener instance), you won't have a MongoDB instance. This means you won't have any product data on your local set up. Though you may populate your postgres database with some predictions, insights, images references, etc. We recommend Factory to create some data in your local database. If you have installed Robotoff via Docker, you can run Python using Poetry and execute Factory like so: $ docker-compose run --rm api poetry run python ... > from tests.integration.models_utils import * > PredictionFactory() NOTE: If you are on Windows we recommend using Git bash to run commands. How to run test cases? We use pytest to run test cases and Makefile to run our commands. The following command will run all the test cases one by one: $ make tests How to run a single test case? The simplest is to use the pytest make target for that: make pytest args = 'path/to/test_file.py::the_function_name' For example, to call test_get_type() from tests/unit/insights/test_importer.py : $ make pytest args = \"tests/unit/insights/test_importer.py::TestLabelInsightImporter::test_get_type\" Remember to put quotes especially if you have multiple arguments. NOTE : Be sure to run make create_external_networks before if needed (especially if you get Network po_test declared as external, but could not be found ) When to write your own test cases? Write test cases every time you write a new feature, to test a feature or to understand the working of an existing function. Automated testing really helps to prevent future bugs as we introduce new features or refactor the code. There are even cases where automated tests are your only chance to test you code. For example: when you write code to post notifications on Slack channel you can only test them by writing a unit test case. There are instances when Robotoff tries to connect to MongoDB via Open Food Facts server. For local testing we do not yet provide a standarized approach to add a MongoDB Docker in the same network and configure Robotoff to use it. In such cases you will have to mock the function which calls MongoDB. Feel free to reuse the existing test cases. To identify parts of the code where Robotoff connects to MongoDB or to Open Food Facts server (the part you should mock), keep an eye for variables like server_url , server_domain or settings.OFF_SERVER_DOMAIN . Debugging guide We encourage using PDB to debug. Running test with --pdb flags, pytest will stop and open the pdb console as soon as there is an error or an assert fails. This can be a good way to try to understand why a test is failing. make pytest args = \"path/to/test.py --pdb\" If it's a mock.assert_called_with , you can look at the real data passed to a test case by calling mock.call_args in the pdb console. If you need more precise control to see code path before it breaks, you can add the following lines in your function to find out what your code does and where it breaks. import pdb ; pdb . set_trace () and then run the pytest , with the --pdb option (as above). Note we need the --pdb option, to view the inputs and outputs captured by pytest and access the pdb console. How to run checks locally When commiting your modifications to the main branch, your code have to pass several tests automatically run by GitHub in order to be merged. You can run theses checks locally before commiting by using the following command: $ make checks","title":"Test and debug"},{"location":"how-to-guides/test-and-debug/#test-and-debug","text":"","title":"Test and debug"},{"location":"how-to-guides/test-and-debug/#this-documentation-intends-to","text":"1) Familiarize you on running our test cases. 2) Give you some tips from getting stuck if you are working only on Robotoff and don't need complete installation of Open Food Facts server.","title":"This documentation intends to:"},{"location":"how-to-guides/test-and-debug/#how-to-generate-data","text":"Robotoff is an API that pulls prediction data, annotation data, product data, nutrition data from MongoDB database in Open Food Facts server. If your development instance is not connected to a product-opener instance (which happens automatically if you have a running product-opener instance), you won't have a MongoDB instance. This means you won't have any product data on your local set up. Though you may populate your postgres database with some predictions, insights, images references, etc. We recommend Factory to create some data in your local database. If you have installed Robotoff via Docker, you can run Python using Poetry and execute Factory like so: $ docker-compose run --rm api poetry run python ... > from tests.integration.models_utils import * > PredictionFactory() NOTE: If you are on Windows we recommend using Git bash to run commands.","title":"How to generate data?"},{"location":"how-to-guides/test-and-debug/#how-to-run-test-cases","text":"We use pytest to run test cases and Makefile to run our commands. The following command will run all the test cases one by one: $ make tests","title":"How to run test cases?"},{"location":"how-to-guides/test-and-debug/#how-to-run-a-single-test-case","text":"The simplest is to use the pytest make target for that: make pytest args = 'path/to/test_file.py::the_function_name' For example, to call test_get_type() from tests/unit/insights/test_importer.py : $ make pytest args = \"tests/unit/insights/test_importer.py::TestLabelInsightImporter::test_get_type\" Remember to put quotes especially if you have multiple arguments. NOTE : Be sure to run make create_external_networks before if needed (especially if you get Network po_test declared as external, but could not be found )","title":"How to run a single test case?"},{"location":"how-to-guides/test-and-debug/#when-to-write-your-own-test-cases","text":"Write test cases every time you write a new feature, to test a feature or to understand the working of an existing function. Automated testing really helps to prevent future bugs as we introduce new features or refactor the code. There are even cases where automated tests are your only chance to test you code. For example: when you write code to post notifications on Slack channel you can only test them by writing a unit test case. There are instances when Robotoff tries to connect to MongoDB via Open Food Facts server. For local testing we do not yet provide a standarized approach to add a MongoDB Docker in the same network and configure Robotoff to use it. In such cases you will have to mock the function which calls MongoDB. Feel free to reuse the existing test cases. To identify parts of the code where Robotoff connects to MongoDB or to Open Food Facts server (the part you should mock), keep an eye for variables like server_url , server_domain or settings.OFF_SERVER_DOMAIN .","title":"When to write your own test cases?"},{"location":"how-to-guides/test-and-debug/#debugging-guide","text":"We encourage using PDB to debug. Running test with --pdb flags, pytest will stop and open the pdb console as soon as there is an error or an assert fails. This can be a good way to try to understand why a test is failing. make pytest args = \"path/to/test.py --pdb\" If it's a mock.assert_called_with , you can look at the real data passed to a test case by calling mock.call_args in the pdb console. If you need more precise control to see code path before it breaks, you can add the following lines in your function to find out what your code does and where it breaks. import pdb ; pdb . set_trace () and then run the pytest , with the --pdb option (as above). Note we need the --pdb option, to view the inputs and outputs captured by pytest and access the pdb console.","title":"Debugging guide"},{"location":"how-to-guides/test-and-debug/#how-to-run-checks-locally","text":"When commiting your modifications to the main branch, your code have to pass several tests automatically run by GitHub in order to be merged. You can run theses checks locally before commiting by using the following command: $ make checks","title":"How to run checks locally"},{"location":"how-to-guides/deployment/dev-install/","text":"Dev install You may choose Docker install (recommended, less chance to mess up with your system, all included) or local install. Docker install After cloning the repository, customize parameters by editing the .env file. You should, consider those changes: if you want to use tensorflow models, add docker/ml.yml to COMPOSE_FILE if you don't work on Product Opener, you should launch MongoDB by adding docker/mongodb.yml to COMPOSE_FILE change OFF_UID and OFF_GID to match your own user UID/GID. (see Getting developper uid for docker ) Note: beware not to commit your changes in the future. Because of Elasticsearch service, you may need to increase a system parameter ( vm.max_map_count=262144 ), as described here . Then simply run: make dev This will build containers, pull images based containers, create containers and run them. It will also download models. Verify whether robotoff is running as expected, by executing the following command in CLI, curl http://robotoff.openfoodfacts.localhost:5500/api/v1/status The expected response is {\"status\":\"running\"} . Also take a look at maintenance Local install with poetry This is an alternative if you are reluctant to use Docker, or have some other reasons to prefer manual install. After cloning the repository: Install the dependencies using Poetry : poetry install Configure files required for the tests to run locally: Compile the i18n files: cd i18n && bash compile.sh && cd .. Also configure your settings to point to your dev postgresql database (that you should have installed the way you want) If you want to use Elasticsearch predictions, you'll also have to set an Elasticsearch instance To configure MongoDB for docker, start with installing official MongoDB image Start the MongoDB container with docker start mongodb Your MongoDB has been installed successfully and is up and running. Congratulations! Once you have the Robotoff services running with docker-compose up along with MongoDB container, proceed with the next steps. Our MongoDB does not have a database right now. So we restore it. tar xzf off-dev-mongo-dump.tar.gz docker cp -a dump robotoff_mongodb_1:/var/tmp/ docker exec mongodb mongorestore /var/tmp/dump First we have extracted the zip file. Next we copy the dump in our running MongoDB service in Robotoff robotoff_mongodb_1 . Lastly, we restore the dump from Robotoff's service to mongodb container. The MongoDB configuration is now complete. You can now work with Robotoff seamlessly. To debug in a running container, you need to run poetry in the container. For example: docker-compose run --rm api poetry run python Here we run the api service. This opens a Python command prompt, you may debug with pdb or play with the code.","title":"Dev install"},{"location":"how-to-guides/deployment/dev-install/#dev-install","text":"You may choose Docker install (recommended, less chance to mess up with your system, all included) or local install.","title":"Dev install"},{"location":"how-to-guides/deployment/dev-install/#docker-install","text":"After cloning the repository, customize parameters by editing the .env file. You should, consider those changes: if you want to use tensorflow models, add docker/ml.yml to COMPOSE_FILE if you don't work on Product Opener, you should launch MongoDB by adding docker/mongodb.yml to COMPOSE_FILE change OFF_UID and OFF_GID to match your own user UID/GID. (see Getting developper uid for docker ) Note: beware not to commit your changes in the future. Because of Elasticsearch service, you may need to increase a system parameter ( vm.max_map_count=262144 ), as described here . Then simply run: make dev This will build containers, pull images based containers, create containers and run them. It will also download models. Verify whether robotoff is running as expected, by executing the following command in CLI, curl http://robotoff.openfoodfacts.localhost:5500/api/v1/status The expected response is {\"status\":\"running\"} . Also take a look at maintenance","title":"Docker install"},{"location":"how-to-guides/deployment/dev-install/#local-install-with-poetry","text":"This is an alternative if you are reluctant to use Docker, or have some other reasons to prefer manual install. After cloning the repository: Install the dependencies using Poetry : poetry install Configure files required for the tests to run locally: Compile the i18n files: cd i18n && bash compile.sh && cd .. Also configure your settings to point to your dev postgresql database (that you should have installed the way you want) If you want to use Elasticsearch predictions, you'll also have to set an Elasticsearch instance To configure MongoDB for docker, start with installing official MongoDB image Start the MongoDB container with docker start mongodb Your MongoDB has been installed successfully and is up and running. Congratulations! Once you have the Robotoff services running with docker-compose up along with MongoDB container, proceed with the next steps. Our MongoDB does not have a database right now. So we restore it. tar xzf off-dev-mongo-dump.tar.gz docker cp -a dump robotoff_mongodb_1:/var/tmp/ docker exec mongodb mongorestore /var/tmp/dump First we have extracted the zip file. Next we copy the dump in our running MongoDB service in Robotoff robotoff_mongodb_1 . Lastly, we restore the dump from Robotoff's service to mongodb container. The MongoDB configuration is now complete. You can now work with Robotoff seamlessly. To debug in a running container, you need to run poetry in the container. For example: docker-compose run --rm api poetry run python Here we run the api service. This opens a Python command prompt, you may debug with pdb or play with the code.","title":"Local install with poetry"},{"location":"how-to-guides/deployment/maintenance/","text":"Services maintenance Robotoff is split in several services: the scheduler , responsible for launching recurrent tasks (downloading new dataset, processing insights automatically,...) the workers , responsible for all long-lasting tasks (mainly insight extraction from images) the public api service the tf_serving service which serve tensor flow models Two additional services are used: a PostgreSQL database ( postgres service) a Elasticsearch single node ( elasticsearch service) All services are managed by docker. docker-compose is used to manage these services. tf-serving has it's own file: docker/ml.yml . Models for tf-serving part are stored as released at https://github.com/openfoodfacts/robotoff-models. Quick start see dev-install You can then use: docker-compose start [service-name] or docker-compose stop [service-name] Or make up when you refresh the product (it will re-build and run docker-compose up -d ). Take the time to become a bit familiar with docker-compose if it's your first use. Monitor To display the logs of the container, docker-compose logs [service-name] . (without service-name, you got all logs). Two options are often used: -f to follow output and --tail n to only display last n lines. To display all running services, run docker-compose ps : Name Command State Ports ---------------------------------------------------------------------------------------------------- robotoff_api_1 /bin/sh -c /docker-entrypo ... Up 0.0.0.0:5500->5500/tcp,:::5500->5500 /tcp robotoff_postgres_1 docker-entrypoint.sh postg ... Up 127.0.0.1:5432->5432/tcp robotoff_scheduler_1 /bin/sh -c /docker-entrypo ... Up robotoff_workers_1 /bin/sh -c /docker-entrypo ... Up Database backup and restore To backup the PostgreSQL database, run the following command: docker exec -i robotoff_postgres_1 pg_dump -U postgres postgres | gzip > $(date +%Y-%m-%d)_robotoff_postgres.sql.gz You can restore it easily locally by running: zcat dump.sql.gz | docker exec -i robotoff_postgres_1 psql -U postgres","title":"Services maintenance"},{"location":"how-to-guides/deployment/maintenance/#services-maintenance","text":"Robotoff is split in several services: the scheduler , responsible for launching recurrent tasks (downloading new dataset, processing insights automatically,...) the workers , responsible for all long-lasting tasks (mainly insight extraction from images) the public api service the tf_serving service which serve tensor flow models Two additional services are used: a PostgreSQL database ( postgres service) a Elasticsearch single node ( elasticsearch service) All services are managed by docker. docker-compose is used to manage these services. tf-serving has it's own file: docker/ml.yml . Models for tf-serving part are stored as released at https://github.com/openfoodfacts/robotoff-models.","title":"Services maintenance"},{"location":"how-to-guides/deployment/maintenance/#quick-start","text":"see dev-install You can then use: docker-compose start [service-name] or docker-compose stop [service-name] Or make up when you refresh the product (it will re-build and run docker-compose up -d ). Take the time to become a bit familiar with docker-compose if it's your first use.","title":"Quick start"},{"location":"how-to-guides/deployment/maintenance/#monitor","text":"To display the logs of the container, docker-compose logs [service-name] . (without service-name, you got all logs). Two options are often used: -f to follow output and --tail n to only display last n lines. To display all running services, run docker-compose ps : Name Command State Ports ---------------------------------------------------------------------------------------------------- robotoff_api_1 /bin/sh -c /docker-entrypo ... Up 0.0.0.0:5500->5500/tcp,:::5500->5500 /tcp robotoff_postgres_1 docker-entrypoint.sh postg ... Up 127.0.0.1:5432->5432/tcp robotoff_scheduler_1 /bin/sh -c /docker-entrypo ... Up robotoff_workers_1 /bin/sh -c /docker-entrypo ... Up","title":"Monitor"},{"location":"how-to-guides/deployment/maintenance/#database-backup-and-restore","text":"To backup the PostgreSQL database, run the following command: docker exec -i robotoff_postgres_1 pg_dump -U postgres postgres | gzip > $(date +%Y-%m-%d)_robotoff_postgres.sql.gz You can restore it easily locally by running: zcat dump.sql.gz | docker exec -i robotoff_postgres_1 psql -U postgres","title":"Database backup and restore"},{"location":"introduction/architecture/","text":"Architecture Robotoff is made of several services: the public API service the scheduler , responsible for launching recurrent tasks (downloading new dataset, processing insights automatically,...) 1 the workers , responsible for all long-lasting tasks Communication between API and Workers happens through ipc events. 2 Robotoff allows to predict many information (also called insights ), mostly from the product images or OCR. Each time a contributor uploads a new image on Open Food Facts, the text on this image is extracted using Google Cloud Vision, an OCR (Optical Character Recognition) service. Robotoff receives a new event through a webhook each time this occurs, with the URLs of the image and the resulting OCR (as a JSON file). We use simple string matching algorithms to find patterns in the OCR text to generate new predictions 3 . We also use a ML model to extract objects from images. 4 One model tries to detect any logo 6 . Detected logos are then embedded in a vector space using a pre-trained model. In this space we use a k-nearest-neighbor approach to try to classify the logo, predicting a brand or a label. Hunger game also collects users annotations to have ground truth ( logo game ). Another model tries to detect the grade of the Nutri-Score (A to E) with a computer vision model. The above detections generate predictions which in turn generate many types of insights 5 : labels stores packager codes packaging product weight expiration date brand ... Predictions, as well as insights are stored in the PostgreSQL database. These new insights are then accessible to all annotation tools (Hunger Games, mobile apps,...), that can validate or not the insight. If the insight is validated by an authenticated user, it's applied immediately and the product is updated through Product Opener API 8 . If it's reported as invalid, no update is performed, but the insight is marked as annotated so that it is not suggested to another annotator. If the user is not authenticated, a system of votes is used (3 consistent votes trigger the insight application). Some insights with high confidence are applied automatically, 10 minutes after import. Robotoff is also notified by Product Opener every time a product is updated or deleted 7 . This is used to delete insights associated with deleted products, or to update them accordingly. Other services Robotoff also depends on the following services: a single node Elasticsearch instance, used to: infer the product category from the product name, using an improved string matching algorithm. 9 (used in conjunction with ML detection) perform spellcheck on ingredient lists 11 a Tensorflow Serving instance, used to serve object detection models (currently, only nutriscore and category), which is identified as robotoff-ml 10 . robotoff-ann which uses an approximate KNN approach to predict logo label MongoDB, to fetch the product latest version without querying Product Opener API. See scheduler.run \u21a9 See robotoff.workers.client and robotoff.workers.listener \u21a9 see robotoff.models.Prediction \u21a9 see robotoff.models.ImagePrediction and robotoff.workers.tasks.import_image.run_import_image_job \u21a9 see robotoff.models.ProductInsight \u21a9 see robotoff.models.ImageAnnotation robotoff.logos \u21a9 see workers.tasks.product_updated and workers.tasks.delete_product_insights \u21a9 see robotoff.insights.annotate \u21a9 see robotoff.elasticsearch.predict \u21a9 see docker/ml.yml \u21a9 see robotoff.spellcheck.elasticsearch.es_handler.ElasticsearchHandler \u21a9","title":"Architecture"},{"location":"introduction/architecture/#architecture","text":"Robotoff is made of several services: the public API service the scheduler , responsible for launching recurrent tasks (downloading new dataset, processing insights automatically,...) 1 the workers , responsible for all long-lasting tasks Communication between API and Workers happens through ipc events. 2 Robotoff allows to predict many information (also called insights ), mostly from the product images or OCR. Each time a contributor uploads a new image on Open Food Facts, the text on this image is extracted using Google Cloud Vision, an OCR (Optical Character Recognition) service. Robotoff receives a new event through a webhook each time this occurs, with the URLs of the image and the resulting OCR (as a JSON file). We use simple string matching algorithms to find patterns in the OCR text to generate new predictions 3 . We also use a ML model to extract objects from images. 4 One model tries to detect any logo 6 . Detected logos are then embedded in a vector space using a pre-trained model. In this space we use a k-nearest-neighbor approach to try to classify the logo, predicting a brand or a label. Hunger game also collects users annotations to have ground truth ( logo game ). Another model tries to detect the grade of the Nutri-Score (A to E) with a computer vision model. The above detections generate predictions which in turn generate many types of insights 5 : labels stores packager codes packaging product weight expiration date brand ... Predictions, as well as insights are stored in the PostgreSQL database. These new insights are then accessible to all annotation tools (Hunger Games, mobile apps,...), that can validate or not the insight. If the insight is validated by an authenticated user, it's applied immediately and the product is updated through Product Opener API 8 . If it's reported as invalid, no update is performed, but the insight is marked as annotated so that it is not suggested to another annotator. If the user is not authenticated, a system of votes is used (3 consistent votes trigger the insight application). Some insights with high confidence are applied automatically, 10 minutes after import. Robotoff is also notified by Product Opener every time a product is updated or deleted 7 . This is used to delete insights associated with deleted products, or to update them accordingly.","title":"Architecture"},{"location":"introduction/architecture/#other-services","text":"Robotoff also depends on the following services: a single node Elasticsearch instance, used to: infer the product category from the product name, using an improved string matching algorithm. 9 (used in conjunction with ML detection) perform spellcheck on ingredient lists 11 a Tensorflow Serving instance, used to serve object detection models (currently, only nutriscore and category), which is identified as robotoff-ml 10 . robotoff-ann which uses an approximate KNN approach to predict logo label MongoDB, to fetch the product latest version without querying Product Opener API. See scheduler.run \u21a9 See robotoff.workers.client and robotoff.workers.listener \u21a9 see robotoff.models.Prediction \u21a9 see robotoff.models.ImagePrediction and robotoff.workers.tasks.import_image.run_import_image_job \u21a9 see robotoff.models.ProductInsight \u21a9 see robotoff.models.ImageAnnotation robotoff.logos \u21a9 see workers.tasks.product_updated and workers.tasks.delete_product_insights \u21a9 see robotoff.insights.annotate \u21a9 see robotoff.elasticsearch.predict \u21a9 see docker/ml.yml \u21a9 see robotoff.spellcheck.elasticsearch.es_handler.ElasticsearchHandler \u21a9","title":"Other services"},{"location":"introduction/contributing/","text":"Contributing Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions Report Bugs Report bugs at https://github.com/openfoodfacts/robotoff/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Issues tagged with \"good first issue\" are suitable for newcomers. Implement Features Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation Robotoff could always use more documentation, whether as part of the official Robotoff docs or in docstrings. Submit Feedback The best way to send feedback is to file an issue at https://github.com/openfoodfacts/robotoff/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome. Get Started! Ready to contribute code? Here's how to set up Robotoff for local development. Fork the robotoff repo on GitHub. Clone your fork locally: git clone git@github.com:your_name_here/robotoff.git 3. choose between docker install (recommended) or local install and run it. code! When you're done making changes, check that your changes pass flake8, mypy and the tests. In addition, ensure that your code is formatted using black: If you are on Windows, make sure you have Make for Windows installed. Don't forget to add its path in your system environment variables . A sample path may look like this: C:\\Program Files (x86)\\GnuWin32\\bin It is recommended to use Window's default command prompt instead of Power shell for smooth installation. If you are using docker: make lint make checks make tests To test the APIs on your localhost run docker-compose up You can make a post request through Postman or simply paste the url in a web browser to make a get request like this one http://localhost:5500/api/v1/insights/ The mapping of functions and API path is at the end of robotoff/app/api.py If you are on a local install: flake8 black --check . mypy . isort --check . poetry run pytest tests Before running the test cases make sure you have a database created. Have a look at .env and robotoff/settings.py the default database name, user, and password is postgres Configure them through environment (you may use .env if you use docker) as you like. We don't provide sample database as of now but you'll have a database structure ready to start working. Commit your changes and push your branch to GitHub: git status git add files-you-have-modified git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature In brief, commit messages should follow these conventions: Always contain a subject line which briefly describes the changes made. For example \"Update CONTRIBUTING.rst\". Subject lines should not exceed 50 characters. The commit body should contain context about the change - how the code worked before, how it works now and why you decided to solve the issue in the way you did. More tips at https://chris.beams.io/posts/git-commit Submit a pull request through the GitHub website. Pull Request Guidelines Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring. The pull request should work for Python 3.7 and above. Check https://github.com/openfoodfacts/robotoff/actions and make sure that the tests pass for all supported Python versions. This contributing page was adapted from Pyswarms documentation .","title":"Contributing"},{"location":"introduction/contributing/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"introduction/contributing/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"introduction/contributing/#report-bugs","text":"Report bugs at https://github.com/openfoodfacts/robotoff/issues . If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"introduction/contributing/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Issues tagged with \"good first issue\" are suitable for newcomers.","title":"Fix Bugs"},{"location":"introduction/contributing/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"introduction/contributing/#write-documentation","text":"Robotoff could always use more documentation, whether as part of the official Robotoff docs or in docstrings.","title":"Write Documentation"},{"location":"introduction/contributing/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/openfoodfacts/robotoff/issues . If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome.","title":"Submit Feedback"},{"location":"introduction/contributing/#get-started","text":"Ready to contribute code? Here's how to set up Robotoff for local development. Fork the robotoff repo on GitHub. Clone your fork locally: git clone git@github.com:your_name_here/robotoff.git 3. choose between docker install (recommended) or local install and run it. code! When you're done making changes, check that your changes pass flake8, mypy and the tests. In addition, ensure that your code is formatted using black: If you are on Windows, make sure you have Make for Windows installed. Don't forget to add its path in your system environment variables . A sample path may look like this: C:\\Program Files (x86)\\GnuWin32\\bin It is recommended to use Window's default command prompt instead of Power shell for smooth installation. If you are using docker: make lint make checks make tests To test the APIs on your localhost run docker-compose up You can make a post request through Postman or simply paste the url in a web browser to make a get request like this one http://localhost:5500/api/v1/insights/ The mapping of functions and API path is at the end of robotoff/app/api.py If you are on a local install: flake8 black --check . mypy . isort --check . poetry run pytest tests Before running the test cases make sure you have a database created. Have a look at .env and robotoff/settings.py the default database name, user, and password is postgres Configure them through environment (you may use .env if you use docker) as you like. We don't provide sample database as of now but you'll have a database structure ready to start working. Commit your changes and push your branch to GitHub: git status git add files-you-have-modified git commit -m \"Your detailed description of your changes.\" git push origin name-of-your-bugfix-or-feature In brief, commit messages should follow these conventions: Always contain a subject line which briefly describes the changes made. For example \"Update CONTRIBUTING.rst\". Subject lines should not exceed 50 characters. The commit body should contain context about the change - how the code worked before, how it works now and why you decided to solve the issue in the way you did. More tips at https://chris.beams.io/posts/git-commit Submit a pull request through the GitHub website.","title":"Get Started!"},{"location":"introduction/contributing/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds functionality, the docs should be updated. Put your new functionality into a function with a docstring. The pull request should work for Python 3.7 and above. Check https://github.com/openfoodfacts/robotoff/actions and make sure that the tests pass for all supported Python versions. This contributing page was adapted from Pyswarms documentation .","title":"Pull Request Guidelines"},{"location":"introduction/notifications/","text":"Robotoff can send notifications to the Open Food Facts Slack workspace 1 . It currently does so in the: nutriscore-alert (for Nutri-Score model predictions) robotoff-alerts-annotations (for logo and labels outputs + manual annotations) robotoff-user-alerts channel (for manual annotations alerts, we've disabled it in 2019) moderation-off-alerts (for problematic images like selfies and nudity) moderation-off-image-alerts (seems a duplication of the previous one) It can also sends images to a moderation service 2 see robotoff.slack.SlackNotifier \u21a9 see robotoff.slack.ImageModerationNotifier \u21a9","title":"Notifications"},{"location":"introduction/notifications/#nutriscore-alert-for-nutri-score-model-predictions","text":"","title":"nutriscore-alert (for Nutri-Score model predictions)"},{"location":"introduction/notifications/#robotoff-alerts-annotations-for-logo-and-labels-outputs-manual-annotations","text":"","title":"robotoff-alerts-annotations (for logo and labels outputs + manual annotations)"},{"location":"introduction/notifications/#robotoff-user-alerts-channel-for-manual-annotations-alerts-weve-disabled-it-in-2019","text":"","title":"robotoff-user-alerts channel (for manual annotations alerts, we've disabled it in 2019)"},{"location":"introduction/notifications/#moderation-off-alerts-for-problematic-images-like-selfies-and-nudity","text":"","title":"moderation-off-alerts (for problematic images like selfies and nudity)"},{"location":"introduction/notifications/#moderation-off-image-alerts-seems-a-duplication-of-the-previous-one","text":"It can also sends images to a moderation service 2 see robotoff.slack.SlackNotifier \u21a9 see robotoff.slack.ImageModerationNotifier \u21a9","title":"moderation-off-image-alerts (seems a duplication of the previous one)"},{"location":"references/cli/","text":"CLI Reference Documentation auto-generated using typer-cli . Usage : $ robotoff-cli [ OPTIONS ] COMMAND [ ARGS ] ... Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : add-logo-to-ann annotate apply-insights batch-annotate categorize : Categorise predicts product categories based... download-dataset download-models : Download model weights from remote URLs. export-logo-annotation generate-ocr-insights : Generate OCR insights of the requested type. generate-spellcheck-insights import-insights : This command is used to backfill a new... import-logos init-elasticsearch : This command is used for manual insertion of... predict-category predict-insight run spellcheck test-spellcheck robotoff-cli add-logo-to-ann Usage : $ robotoff-cli add-logo-to-ann [ OPTIONS ] Options : --sleep-time FLOAT : [default: 0.5] --help : Show this message and exit. robotoff-cli annotate Usage : $ robotoff-cli annotate [ OPTIONS ] INSIGHT_TYPE COUNTRY Arguments : INSIGHT_TYPE : [required] COUNTRY : [required] Options : --help : Show this message and exit. robotoff-cli apply-insights Usage : $ robotoff-cli apply-insights [ OPTIONS ] INSIGHT_TYPE Arguments : INSIGHT_TYPE : [required] Options : --delta INTEGER : [default: 1] --help : Show this message and exit. robotoff-cli batch-annotate Usage : $ robotoff-cli batch-annotate [ OPTIONS ] INSIGHT_TYPE FILTER_CLAUSE Arguments : INSIGHT_TYPE : [required] FILTER_CLAUSE : [required] Options : --dry / --no-dry : [default: True] --help : Show this message and exit. robotoff-cli categorize Categorise predicts product categories based on the neural category classifier. deepest_only: controls whether the returned predictions should only contain the deepmost categories for a predicted taxonomy chain. For example, if we predict 'fresh vegetables' -> 'legumes' -> 'beans' for a product, setting deepest_only=True will return 'beans'. Usage : $ robotoff-cli categorize [ OPTIONS ] BARCODE Arguments : BARCODE : [required] Options : --deepest-only / --no-deepest-only : [default: False] --help : Show this message and exit. robotoff-cli download-dataset Usage : $ robotoff-cli download-dataset [ OPTIONS ] Options : --minify / --no-minify : [default: False] --help : Show this message and exit. robotoff-cli download-models Download model weights from remote URLs. If models have already been downloaded, the command is skipped unless --force option is used. Usage : $ robotoff-cli download-models [ OPTIONS ] Options : --force / --no-force : [default: False] --help : Show this message and exit. robotoff-cli export-logo-annotation Usage : $ robotoff-cli export-logo-annotation [ OPTIONS ] OUTPUT Arguments : OUTPUT : [required] Options : --server-domain TEXT --annotated / --no-annotated --help : Show this message and exit. robotoff-cli generate-ocr-insights Generate OCR insights of the requested type. SOURCE can be either: * the path to a JSON file, a (gzipped-)JSONL file, or a directory containing JSON files * a barcode * the '-' character: input is read from stdin and assumed to be JSONL Output is JSONL, each line containing the insights for one document. Usage : $ robotoff-cli generate-ocr-insights [ OPTIONS ] SOURCE PREDICTION_TYPE Arguments : SOURCE : [required] PREDICTION_TYPE : [required] Options : --output FILE : File to write output to, stdout if not specified [required] --help : Show this message and exit. robotoff-cli generate-spellcheck-insights Usage : $ robotoff-cli generate-spellcheck-insights [ OPTIONS ] OUTPUT Arguments : OUTPUT : [required] Options : --index-name TEXT : [default: product_all] --confidence FLOAT : [default: 0.5] --max-errors INTEGER --limit INTEGER --help : Show this message and exit. robotoff-cli import-insights This command is used to backfill a new insight type on the daily product data dump. Usage : $ robotoff-cli import-insights [ OPTIONS ] INSIGHT_TYPE Arguments : INSIGHT_TYPE : [required] Options : --server-domain TEXT --batch-size INTEGER : [default: 1024] --input- PATH --generate-from PATH --help : Show this message and exit. robotoff-cli import-logos Usage : $ robotoff-cli import-logos [ OPTIONS ] DATA_PATH Arguments : DATA_PATH : [required] Options : --model-name TEXT : [default: universal-logo-detector] --model-version TEXT : [default: tf-universal-logo-detector-1.0] --help : Show this message and exit. robotoff-cli init-elasticsearch This command is used for manual insertion of the Elasticsearch data and/or indexes for products and categorties. to_load specifies which indexes/data should be loaded - supported values are in robotoff.settings.ElasticsearchIndex. Usage : $ robotoff-cli init-elasticsearch [ OPTIONS ] Options : --load-index / --no-load-index : [default: False] --load-data / --no-load-data : [default: True] --to-load TEXT --help : Show this message and exit. robotoff-cli predict-category Usage : $ robotoff-cli predict-category [ OPTIONS ] OUTPUT Arguments : OUTPUT : [required] Options : --help : Show this message and exit. robotoff-cli predict-insight Usage : $ robotoff-cli predict-insight [ OPTIONS ] OCR_URL Arguments : OCR_URL : [required] Options : --help : Show this message and exit. robotoff-cli run Usage : $ robotoff-cli run [ OPTIONS ] SERVICE Arguments : SERVICE : [required] Options : --help : Show this message and exit. robotoff-cli spellcheck Usage : $ robotoff-cli spellcheck [ OPTIONS ] PATTERN CORRECTION Arguments : PATTERN : [required] CORRECTION : [required] Options : --country TEXT : [default: fr] --dry / --no-dry : [default: False] --help : Show this message and exit. robotoff-cli test-spellcheck Usage : $ robotoff-cli test-spellcheck [ OPTIONS ] TEXT Arguments : TEXT : [required] Options : --confidence FLOAT : [default: 1.0] --help : Show this message and exit.","title":"CLI Reference"},{"location":"references/cli/#cli-reference","text":"Documentation auto-generated using typer-cli . Usage : $ robotoff-cli [ OPTIONS ] COMMAND [ ARGS ] ... Options : --install-completion : Install completion for the current shell. --show-completion : Show completion for the current shell, to copy it or customize the installation. --help : Show this message and exit. Commands : add-logo-to-ann annotate apply-insights batch-annotate categorize : Categorise predicts product categories based... download-dataset download-models : Download model weights from remote URLs. export-logo-annotation generate-ocr-insights : Generate OCR insights of the requested type. generate-spellcheck-insights import-insights : This command is used to backfill a new... import-logos init-elasticsearch : This command is used for manual insertion of... predict-category predict-insight run spellcheck test-spellcheck","title":"CLI Reference"},{"location":"references/cli/#robotoff-cli-add-logo-to-ann","text":"Usage : $ robotoff-cli add-logo-to-ann [ OPTIONS ] Options : --sleep-time FLOAT : [default: 0.5] --help : Show this message and exit.","title":"robotoff-cli add-logo-to-ann"},{"location":"references/cli/#robotoff-cli-annotate","text":"Usage : $ robotoff-cli annotate [ OPTIONS ] INSIGHT_TYPE COUNTRY Arguments : INSIGHT_TYPE : [required] COUNTRY : [required] Options : --help : Show this message and exit.","title":"robotoff-cli annotate"},{"location":"references/cli/#robotoff-cli-apply-insights","text":"Usage : $ robotoff-cli apply-insights [ OPTIONS ] INSIGHT_TYPE Arguments : INSIGHT_TYPE : [required] Options : --delta INTEGER : [default: 1] --help : Show this message and exit.","title":"robotoff-cli apply-insights"},{"location":"references/cli/#robotoff-cli-batch-annotate","text":"Usage : $ robotoff-cli batch-annotate [ OPTIONS ] INSIGHT_TYPE FILTER_CLAUSE Arguments : INSIGHT_TYPE : [required] FILTER_CLAUSE : [required] Options : --dry / --no-dry : [default: True] --help : Show this message and exit.","title":"robotoff-cli batch-annotate"},{"location":"references/cli/#robotoff-cli-categorize","text":"Categorise predicts product categories based on the neural category classifier. deepest_only: controls whether the returned predictions should only contain the deepmost categories for a predicted taxonomy chain. For example, if we predict 'fresh vegetables' -> 'legumes' -> 'beans' for a product, setting deepest_only=True will return 'beans'. Usage : $ robotoff-cli categorize [ OPTIONS ] BARCODE Arguments : BARCODE : [required] Options : --deepest-only / --no-deepest-only : [default: False] --help : Show this message and exit.","title":"robotoff-cli categorize"},{"location":"references/cli/#robotoff-cli-download-dataset","text":"Usage : $ robotoff-cli download-dataset [ OPTIONS ] Options : --minify / --no-minify : [default: False] --help : Show this message and exit.","title":"robotoff-cli download-dataset"},{"location":"references/cli/#robotoff-cli-download-models","text":"Download model weights from remote URLs. If models have already been downloaded, the command is skipped unless --force option is used. Usage : $ robotoff-cli download-models [ OPTIONS ] Options : --force / --no-force : [default: False] --help : Show this message and exit.","title":"robotoff-cli download-models"},{"location":"references/cli/#robotoff-cli-export-logo-annotation","text":"Usage : $ robotoff-cli export-logo-annotation [ OPTIONS ] OUTPUT Arguments : OUTPUT : [required] Options : --server-domain TEXT --annotated / --no-annotated --help : Show this message and exit.","title":"robotoff-cli export-logo-annotation"},{"location":"references/cli/#robotoff-cli-generate-ocr-insights","text":"Generate OCR insights of the requested type. SOURCE can be either: * the path to a JSON file, a (gzipped-)JSONL file, or a directory containing JSON files * a barcode * the '-' character: input is read from stdin and assumed to be JSONL Output is JSONL, each line containing the insights for one document. Usage : $ robotoff-cli generate-ocr-insights [ OPTIONS ] SOURCE PREDICTION_TYPE Arguments : SOURCE : [required] PREDICTION_TYPE : [required] Options : --output FILE : File to write output to, stdout if not specified [required] --help : Show this message and exit.","title":"robotoff-cli generate-ocr-insights"},{"location":"references/cli/#robotoff-cli-generate-spellcheck-insights","text":"Usage : $ robotoff-cli generate-spellcheck-insights [ OPTIONS ] OUTPUT Arguments : OUTPUT : [required] Options : --index-name TEXT : [default: product_all] --confidence FLOAT : [default: 0.5] --max-errors INTEGER --limit INTEGER --help : Show this message and exit.","title":"robotoff-cli generate-spellcheck-insights"},{"location":"references/cli/#robotoff-cli-import-insights","text":"This command is used to backfill a new insight type on the daily product data dump. Usage : $ robotoff-cli import-insights [ OPTIONS ] INSIGHT_TYPE Arguments : INSIGHT_TYPE : [required] Options : --server-domain TEXT --batch-size INTEGER : [default: 1024] --input- PATH --generate-from PATH --help : Show this message and exit.","title":"robotoff-cli import-insights"},{"location":"references/cli/#robotoff-cli-import-logos","text":"Usage : $ robotoff-cli import-logos [ OPTIONS ] DATA_PATH Arguments : DATA_PATH : [required] Options : --model-name TEXT : [default: universal-logo-detector] --model-version TEXT : [default: tf-universal-logo-detector-1.0] --help : Show this message and exit.","title":"robotoff-cli import-logos"},{"location":"references/cli/#robotoff-cli-init-elasticsearch","text":"This command is used for manual insertion of the Elasticsearch data and/or indexes for products and categorties. to_load specifies which indexes/data should be loaded - supported values are in robotoff.settings.ElasticsearchIndex. Usage : $ robotoff-cli init-elasticsearch [ OPTIONS ] Options : --load-index / --no-load-index : [default: False] --load-data / --no-load-data : [default: True] --to-load TEXT --help : Show this message and exit.","title":"robotoff-cli init-elasticsearch"},{"location":"references/cli/#robotoff-cli-predict-category","text":"Usage : $ robotoff-cli predict-category [ OPTIONS ] OUTPUT Arguments : OUTPUT : [required] Options : --help : Show this message and exit.","title":"robotoff-cli predict-category"},{"location":"references/cli/#robotoff-cli-predict-insight","text":"Usage : $ robotoff-cli predict-insight [ OPTIONS ] OCR_URL Arguments : OCR_URL : [required] Options : --help : Show this message and exit.","title":"robotoff-cli predict-insight"},{"location":"references/cli/#robotoff-cli-run","text":"Usage : $ robotoff-cli run [ OPTIONS ] SERVICE Arguments : SERVICE : [required] Options : --help : Show this message and exit.","title":"robotoff-cli run"},{"location":"references/cli/#robotoff-cli-spellcheck","text":"Usage : $ robotoff-cli spellcheck [ OPTIONS ] PATTERN CORRECTION Arguments : PATTERN : [required] CORRECTION : [required] Options : --country TEXT : [default: fr] --dry / --no-dry : [default: False] --help : Show this message and exit.","title":"robotoff-cli spellcheck"},{"location":"references/cli/#robotoff-cli-test-spellcheck","text":"Usage : $ robotoff-cli test-spellcheck [ OPTIONS ] TEXT Arguments : TEXT : [required] Options : --confidence FLOAT : [default: 1.0] --help : Show this message and exit.","title":"robotoff-cli test-spellcheck"},{"location":"references/package/","text":"Package reference Robotoff is deployed on a web server, but is also distributed as a library. This document presents a brief, high-level overview of Robotoff\u2019s library primary features. This guide will cover: Using OFF dataset Using the taxonomies (ingredient, label, category) Install Robotoff is currently compatible with Python 3.7 and 3.8. Robotoff can be installed following dev install doc Play with the Open Food Facts dataset First, download the dataset: robotoff-cli download-dataset Robotoff includes a set of tools to easily handle the OFF dataset. As an example, we can print the product name of all complete products from France that have ingredients in French with: from robotoff.products import ProductDataset ds = ProductDataset . load () product_iter = ( ds . stream () . filter_by_country_tag ( 'en:france' ) . filter_nonempty_text_field ( 'ingredients_text_fr' ) . filter_by_state_tag ( 'en:complete' ) . iter ()) for product in product_iter : print ( product [ 'product_name' ]) We first lazily load the dataset using ProductDataset.load() . Then, we create a ProductStream using the ProductDataset.stream() method, and apply filters on the stream of products. The following filters are currently available: filter_by_country_tag filter_by_state_tag filter_nonempty_text_field filter_empty_text_field filter_nonempty_tag_field filter_empty_tag_field filter_by_modified_datetime Play with the taxonomies Taxonomies contains items (such as ingredients, labels or categories) organized in a hierarchical way. Some items are children of other items. For instance, en:brown-rice is a child of en:rice . from robotoff.taxonomy import get_taxonomy # supported taxonomies: ingredient, category, label taxonomy = get_taxonomy ( 'category' ) brown_rice = taxonomy [ 'en:brown-rices' ] rice = taxonomy [ 'en:rices' ] print ( brown_rice ) # Output: <TaxonomyNode en:brown-rices> print ( brown_rice . children ) # Output: [<TaxonomyNode en:brown-jasmine-rices>, <TaxonomyNode en:brown-basmati-rices>] assert brown_rice . is_child_of ( rice ) assert rice . is_parent_of ( brown_rice ) assert brown_rice . get_localized_name ( 'fr' ) == 'Riz complet' # find_deepest_item takes a list of string as input and outputs a string deepest_node = taxonomy . find_deepest_nodes ([ rice , brown_rice ]) assert deepest_node == [ brown_rice ] print ( brown_rice . get_synonyms ( 'fr' )) # Output: ['Riz complet', 'riz cargo', 'riz brun', 'riz semi-complet'] print ( brown_rice . get_parents_hierarchy ()) # Output: [<TaxonomyNode en:rices>, <TaxonomyNode en:cereal-grains>, <TaxonomyNode en:cereals-and-their-products>, <TaxonomyNode en:cereals-and-potatoes>, <TaxonomyNode en:plant-based-foods>, <TaxonomyNode en:plant-based-foods-and-beverages>, <TaxonomyNode en:seeds>]","title":"Package reference"},{"location":"references/package/#package-reference","text":"Robotoff is deployed on a web server, but is also distributed as a library. This document presents a brief, high-level overview of Robotoff\u2019s library primary features. This guide will cover: Using OFF dataset Using the taxonomies (ingredient, label, category)","title":"Package reference"},{"location":"references/package/#install","text":"Robotoff is currently compatible with Python 3.7 and 3.8. Robotoff can be installed following dev install doc","title":"Install"},{"location":"references/package/#play-with-the-open-food-facts-dataset","text":"First, download the dataset: robotoff-cli download-dataset Robotoff includes a set of tools to easily handle the OFF dataset. As an example, we can print the product name of all complete products from France that have ingredients in French with: from robotoff.products import ProductDataset ds = ProductDataset . load () product_iter = ( ds . stream () . filter_by_country_tag ( 'en:france' ) . filter_nonempty_text_field ( 'ingredients_text_fr' ) . filter_by_state_tag ( 'en:complete' ) . iter ()) for product in product_iter : print ( product [ 'product_name' ]) We first lazily load the dataset using ProductDataset.load() . Then, we create a ProductStream using the ProductDataset.stream() method, and apply filters on the stream of products. The following filters are currently available: filter_by_country_tag filter_by_state_tag filter_nonempty_text_field filter_empty_text_field filter_nonempty_tag_field filter_empty_tag_field filter_by_modified_datetime","title":"Play with the Open Food Facts dataset"},{"location":"references/package/#play-with-the-taxonomies","text":"Taxonomies contains items (such as ingredients, labels or categories) organized in a hierarchical way. Some items are children of other items. For instance, en:brown-rice is a child of en:rice . from robotoff.taxonomy import get_taxonomy # supported taxonomies: ingredient, category, label taxonomy = get_taxonomy ( 'category' ) brown_rice = taxonomy [ 'en:brown-rices' ] rice = taxonomy [ 'en:rices' ] print ( brown_rice ) # Output: <TaxonomyNode en:brown-rices> print ( brown_rice . children ) # Output: [<TaxonomyNode en:brown-jasmine-rices>, <TaxonomyNode en:brown-basmati-rices>] assert brown_rice . is_child_of ( rice ) assert rice . is_parent_of ( brown_rice ) assert brown_rice . get_localized_name ( 'fr' ) == 'Riz complet' # find_deepest_item takes a list of string as input and outputs a string deepest_node = taxonomy . find_deepest_nodes ([ rice , brown_rice ]) assert deepest_node == [ brown_rice ] print ( brown_rice . get_synonyms ( 'fr' )) # Output: ['Riz complet', 'riz cargo', 'riz brun', 'riz semi-complet'] print ( brown_rice . get_parents_hierarchy ()) # Output: [<TaxonomyNode en:rices>, <TaxonomyNode en:cereal-grains>, <TaxonomyNode en:cereals-and-their-products>, <TaxonomyNode en:cereals-and-potatoes>, <TaxonomyNode en:plant-based-foods>, <TaxonomyNode en:plant-based-foods-and-beverages>, <TaxonomyNode en:seeds>]","title":"Play with the taxonomies"},{"location":"research/","text":"Overview This page lists the different research projects conducted by the Open Food Facts team and volunteers. Nutrition table detection","title":"Overview"},{"location":"research/#overview","text":"This page lists the different research projects conducted by the Open Food Facts team and volunteers. Nutrition table detection","title":"Overview"},{"location":"research/logo-detection/benchmark/","text":"Benchmark When releasing the first version of the logo detection pipeline, the performance was unknown, as we didn't have any labeled logo dataset to measure it. This pipeline helped us build the first annotated logo dataset so that we can now measure how the original EfficientNet-b0 model (pretrained on ImageNet) performs compared to other pretrained models. Logo embeddings were computed using each model. For each logo, two different distances were used to find the most similar logos among the rest of the dataset, and results were sorted by ascending distance. The first table shows the results obtained with the L2 distance and the second one shows the results we got with the cosine distance. To keep the comparison fair and avoid favoring classes with many samples, for each target image, we only considered at most 4 items of each class. These items were sampled at random among the class items. As each class contains at least 5 items, all classes (including the target class, i.e. the class of the target logo) have 4 candidates. With this setting, an oracle model would have a recall@4 of 1. The val split was used to perform this benchmark. The benchmark code can be found here . We use the following metrics: micro-recall@4 : skewed by classes with many samples. macro-recall@4 : gives equal weight to all classes. All compared models were trained on ImageNet, except: beit_large_patch16_224_in22k , trained on ImageNet 22k clip-vit-* , trained on the proprietary dataset described in the CLIP paper . Note that we use the timm library to generate embeddings (except for CLIP models, where the transformers library was used). The model weights mostly come from the timm author's training and differ from the original weights. Latency was measured on 50 batches of 8 samples with a Tesla T4 GPU. With L2 distance : model micro-recall@4 macro-recall@4 random 0.0083 0.0063 efficientnet_b1 0.4612 0.5070 resnest101e 0.4322 0.5124 beit_large_patch16_384 0.4162 0.5233 efficientnet_b2 0.4707 0.5323 rexnet_100 0.5158 0.5340 efficientnet_b4 0.4807 0.5450 resnet50 0.4916 0.5609 efficientnet_b0 0.5420 0.5665 beit_base_patch16_384 0.4758 0.5666 resnet50d 0.5313 0.6133 beit_large_patch16_224_in22k 0.5723 0.6660 clip-vit-base-patch32 0.7006 0.8243 clip-vit-base-patch16 0.7295 0.8428 clip-vit-large-patch14 0.7706 0.8755 deit_base_patch16_384 0.3920 0.4375 With cosine distance : model micro-recall@4 macro-recall@4 random 0.0091 0.0114 efficientnet_b1 0.4941 0.5334 resnest101e 0.4777 0.5415 efficientnet_b2 0.4909 0.5466 rexnet_100 0.5461 0.5716 efficientnet_b4 0.5141 0.5861 resnet50 0.5241 0.5868 efficientnet_b0 0.5489 0.5838 resnet50d 0.5791 0.6353 clip-vit-base-patch32 0.7030 0.8244 clip-vit-base-patch16 0.7297 0.8470 clip-vit-large-patch14 0.7753 0.8722 deit_base_patch16_384 0.4403 0.5070 Embedding size and per-sample latency (independent of the distance used): model embedding size per-sample latency (ms) random - - efficientnet_b1 1280 3.93 resnest101e 1024 142.75 efficientnet_b2 1408 4.29 rexnet_100 1280 3.91 efficientnet_b4 1792 6.99 resnet50 2048 3.50 efficientnet_b0 1280 5.51 beit_base_patch16_384 768 41.88 resnet50d 2048 4.01 beit_large_patch16_224_in22k 1024 43.56 clip-vit-base-patch32 768 3.08 clip-vit-base-patch16 768 11.69 clip-vit-large-patch14 1024 56.68 deit_base_patch16_384 80 1.73 N.B: we didn't use the cosine-distance for the beit models as they were not working anymore when doing the benchmark with the cosine distance. Some explanations can be found there . We note that the cosine distance tends to be slightly better than the L2 distance for each model (except for the clip-vit-large-patch14 ) but the use of one distance or the other does not change the following analysis as the ranking of the models remains the same. As expected, the current model ( efficientnet-b0 ) performs well above the random baseline. Its performances are competitive compared to most other architectures pretrained on ImageNet. However, CLIP models largely outperform any other tested architecture on this benchmark: with clip-vit-large-patch14 we gain +22.8 on micro-recall@4 and +30.9 on macro-recall@4 compared to efficientnet-b0 . Performances of CLIP models increase as models gets larger or with a smaller image patch size. The prediction latency is however 3.8x and 18,4x higher for clip-vit-base-patch16 and clip-vit-large-patch14 respectively compared to clip-vit-base-patch32 . In conclusion, CLIP models are very good candidates for an off-the-shelf replacement of the efficientnet-b0 model currently used to generate logo embeddings. An additional benefit from this model architecture is the smaller embedding size (768 or 1024, depending on the version) compared to the original efficientnet-b0 model (1280).","title":"Benchmark"},{"location":"research/logo-detection/benchmark/#benchmark","text":"When releasing the first version of the logo detection pipeline, the performance was unknown, as we didn't have any labeled logo dataset to measure it. This pipeline helped us build the first annotated logo dataset so that we can now measure how the original EfficientNet-b0 model (pretrained on ImageNet) performs compared to other pretrained models. Logo embeddings were computed using each model. For each logo, two different distances were used to find the most similar logos among the rest of the dataset, and results were sorted by ascending distance. The first table shows the results obtained with the L2 distance and the second one shows the results we got with the cosine distance. To keep the comparison fair and avoid favoring classes with many samples, for each target image, we only considered at most 4 items of each class. These items were sampled at random among the class items. As each class contains at least 5 items, all classes (including the target class, i.e. the class of the target logo) have 4 candidates. With this setting, an oracle model would have a recall@4 of 1. The val split was used to perform this benchmark. The benchmark code can be found here . We use the following metrics: micro-recall@4 : skewed by classes with many samples. macro-recall@4 : gives equal weight to all classes. All compared models were trained on ImageNet, except: beit_large_patch16_224_in22k , trained on ImageNet 22k clip-vit-* , trained on the proprietary dataset described in the CLIP paper . Note that we use the timm library to generate embeddings (except for CLIP models, where the transformers library was used). The model weights mostly come from the timm author's training and differ from the original weights. Latency was measured on 50 batches of 8 samples with a Tesla T4 GPU. With L2 distance : model micro-recall@4 macro-recall@4 random 0.0083 0.0063 efficientnet_b1 0.4612 0.5070 resnest101e 0.4322 0.5124 beit_large_patch16_384 0.4162 0.5233 efficientnet_b2 0.4707 0.5323 rexnet_100 0.5158 0.5340 efficientnet_b4 0.4807 0.5450 resnet50 0.4916 0.5609 efficientnet_b0 0.5420 0.5665 beit_base_patch16_384 0.4758 0.5666 resnet50d 0.5313 0.6133 beit_large_patch16_224_in22k 0.5723 0.6660 clip-vit-base-patch32 0.7006 0.8243 clip-vit-base-patch16 0.7295 0.8428 clip-vit-large-patch14 0.7706 0.8755 deit_base_patch16_384 0.3920 0.4375 With cosine distance : model micro-recall@4 macro-recall@4 random 0.0091 0.0114 efficientnet_b1 0.4941 0.5334 resnest101e 0.4777 0.5415 efficientnet_b2 0.4909 0.5466 rexnet_100 0.5461 0.5716 efficientnet_b4 0.5141 0.5861 resnet50 0.5241 0.5868 efficientnet_b0 0.5489 0.5838 resnet50d 0.5791 0.6353 clip-vit-base-patch32 0.7030 0.8244 clip-vit-base-patch16 0.7297 0.8470 clip-vit-large-patch14 0.7753 0.8722 deit_base_patch16_384 0.4403 0.5070 Embedding size and per-sample latency (independent of the distance used): model embedding size per-sample latency (ms) random - - efficientnet_b1 1280 3.93 resnest101e 1024 142.75 efficientnet_b2 1408 4.29 rexnet_100 1280 3.91 efficientnet_b4 1792 6.99 resnet50 2048 3.50 efficientnet_b0 1280 5.51 beit_base_patch16_384 768 41.88 resnet50d 2048 4.01 beit_large_patch16_224_in22k 1024 43.56 clip-vit-base-patch32 768 3.08 clip-vit-base-patch16 768 11.69 clip-vit-large-patch14 1024 56.68 deit_base_patch16_384 80 1.73 N.B: we didn't use the cosine-distance for the beit models as they were not working anymore when doing the benchmark with the cosine distance. Some explanations can be found there . We note that the cosine distance tends to be slightly better than the L2 distance for each model (except for the clip-vit-large-patch14 ) but the use of one distance or the other does not change the following analysis as the ranking of the models remains the same. As expected, the current model ( efficientnet-b0 ) performs well above the random baseline. Its performances are competitive compared to most other architectures pretrained on ImageNet. However, CLIP models largely outperform any other tested architecture on this benchmark: with clip-vit-large-patch14 we gain +22.8 on micro-recall@4 and +30.9 on macro-recall@4 compared to efficientnet-b0 . Performances of CLIP models increase as models gets larger or with a smaller image patch size. The prediction latency is however 3.8x and 18,4x higher for clip-vit-base-patch16 and clip-vit-large-patch14 respectively compared to clip-vit-base-patch32 . In conclusion, CLIP models are very good candidates for an off-the-shelf replacement of the efficientnet-b0 model currently used to generate logo embeddings. An additional benefit from this model architecture is the smaller embedding size (768 or 1024, depending on the version) compared to the original efficientnet-b0 model (1280).","title":"Benchmark"},{"location":"research/nutrition-table-detection/","text":"Overview The annotation guideline that was used to annotate the nutrition table can be found here .","title":"Overview"},{"location":"research/nutrition-table-detection/#overview","text":"The annotation guideline that was used to annotate the nutrition table can be found here .","title":"Overview"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/","text":"Nutrition Table Annotation Guidelines Guidelines on what and how to label. Adapted from http://host.robots.ox.ac.uk/pascal/VOC/voc2011/guidelines.html What to label All objects of the defined categories, unless: you are unsure what the object is. the object is very small (at your discretion). less than 10-20% of the object is visible, such that you cannot be sure what class it is. Bounding box Mark the bounding box of the visible area of the object (not the estimated total extent of the object). Bounding box should contain all visible pixels. The bounding box should enclose the object as tight as possible. Clothing/mud/ snow etc. If an object is \u2018occluded\u2019 by a close-fitting occluder e.g. clothing, mud, snow etc., then the occluder should be treated as part of the object. Transparency Do label objects visible through glass, but treat reflections on the glass as occlusion. Mirrors Do label objects in mirrors. Pictures Label objects in pictures/posters/signs only if they are photorealistic but not if cartoons, symbols etc. Guidelines on categorization nutrition-table : a table containing nutrition facts. nutrition-table-text : variant where the nutrition facts are not displayed in a table. Ex: Nutritional facts for 100g: Energy - 252 kJ, fat: 12g,... . nutrition-table-small-energy : symbol often found on the front image of the product, indicating the kJ/kcal of a portion/100g of the product. The bounding box should only enclose the symbol, and not additional texts around it. Do not use this label if other nutritional information are layed out next to the object, see nutrition-table-small. nutrition-table-small : pack of symbols often found on the front image of the product, indicating the nutrition facts. If there are several nutrition-table or nutrition-table-text on the image (often found on multilingual products), label each object.","title":"Nutrition Table Annotation Guidelines"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#nutrition-table-annotation-guidelines","text":"Guidelines on what and how to label. Adapted from http://host.robots.ox.ac.uk/pascal/VOC/voc2011/guidelines.html","title":"Nutrition Table Annotation Guidelines"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#what-to-label","text":"All objects of the defined categories, unless: you are unsure what the object is. the object is very small (at your discretion). less than 10-20% of the object is visible, such that you cannot be sure what class it is.","title":"What to label"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#bounding-box","text":"Mark the bounding box of the visible area of the object (not the estimated total extent of the object). Bounding box should contain all visible pixels. The bounding box should enclose the object as tight as possible.","title":"Bounding box"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#clothingmud-snow-etc","text":"If an object is \u2018occluded\u2019 by a close-fitting occluder e.g. clothing, mud, snow etc., then the occluder should be treated as part of the object.","title":"Clothing/mud/ snow etc."},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#transparency","text":"Do label objects visible through glass, but treat reflections on the glass as occlusion.","title":"Transparency"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#mirrors","text":"Do label objects in mirrors.","title":"Mirrors"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#pictures","text":"Label objects in pictures/posters/signs only if they are photorealistic but not if cartoons, symbols etc.","title":"Pictures"},{"location":"research/nutrition-table-detection/nutrition-tables-annotation-guidelines/#guidelines-on-categorization","text":"nutrition-table : a table containing nutrition facts. nutrition-table-text : variant where the nutrition facts are not displayed in a table. Ex: Nutritional facts for 100g: Energy - 252 kJ, fat: 12g,... . nutrition-table-small-energy : symbol often found on the front image of the product, indicating the kJ/kcal of a portion/100g of the product. The bounding box should only enclose the symbol, and not additional texts around it. Do not use this label if other nutritional information are layed out next to the object, see nutrition-table-small. nutrition-table-small : pack of symbols often found on the front image of the product, indicating the nutrition facts. If there are several nutrition-table or nutrition-table-text on the image (often found on multilingual products), label each object.","title":"Guidelines on categorization"}]}